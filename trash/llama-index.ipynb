{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Llama-index FYP\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/fyp_goo/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/workspaces/fyp_goo/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/workspaces/fyp_goo/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\n",
      "\n",
      "Only extract the properties mentioned in the 'information_extraction' function.\n",
      "\n",
      "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
      "\n",
      "Passage:\n",
      "[Document(page_content=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0}), Document(page_content='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 1}), Document(page_content='', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 2})]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Document(page_content='name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: 1 year\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Data Mining, Relational Database, Deep Learning\\nRelevant experience: Data Scientist with experience in data mining, machine learning, and database management\\n', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addf84f2-d5fd-11ee-9f57-000d3ac8563d']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from dataclasses import dataclass\n",
    "chroma_directory = 'db/'\n",
    "\n",
    "db = Chroma(persist_directory=chroma_directory, embedding_function=OpenAIEmbeddings())\n",
    "#file_path = \"JiaHao_Lo.pdf\"\n",
    "#file_path = \"GooYeJui_CV.pdf\"\n",
    "file_path = \"resume/Ang Teik Hun Resume.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "results = extract_information(documents)\n",
    "\n",
    "document_objects = []\n",
    "# Construct page content from dictionary values\n",
    "page_content = \"\"\n",
    "for key, value in results[0].items():\n",
    "    page_content += f\"{key}: {value}\\n\"\n",
    "\n",
    "# Construct metadata\n",
    "metadata = {'source': file_path, 'page': 0}  # You may adjust the source accordingly\n",
    "\n",
    "# Create Document object\n",
    "document = Document(page_content=page_content, metadata=metadata)\n",
    "document_objects.append(document)\n",
    "\n",
    "print(document_objects)\n",
    "db.add_documents(documents=document_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pack\n",
    "job_description = \"\"\"Responsible for design, planning, and coordinating the implementation of Data Science work activities in the Group Digital with established structured processes and procedures to support PETRONAS's digital agenda.\n",
    " \n",
    "\n",
    "1) Technical & Professional Excellence\n",
    "\n",
    "Responsible for ensuring data required for analytic models is of required quality and models are constructed to standards and deployed effectively.\n",
    "Implement data science industry best practices, relevant cutting-edge technology, and innovation in projects to ensure the right solutions fulfill the business requirements.\n",
    "\n",
    "2) Technical/Skill Leadership & Solutioning\n",
    "\n",
    "Responsible for developing appropriate technical solutions to address business pain points with insights and recommendations.\n",
    "Implement an established analytics strategy by adopting the right technologies and technical requirements when executing projects to ensure business value generation.\n",
    "Execute operational excellence through continuous technical and process improvement initiatives within projects to improve operations efficiency and effectiveness within own activity & projects.\n",
    "\n",
    "3) Technical Expertise\n",
    "\n",
    "Track and follow up with relevant parties to ensure Technical Excellence Programmes are successfully deployed and integrated into work processes, documents, policies, and guidelines.\n",
    "Participate in a community of practices and network with internal and external technical experts by identifying solutions to common problems and capturing and sharing existing data science knowledge for continuous excellence.\n",
    "\n",
    " \n",
    "\n",
    "Be part of our DS team in at least one of the following areas:\n",
    "\n",
    "Machine Learning\n",
    "\n",
    "Roles: Design analytics solutions for business problems; develop, evaluate, optimize, deploy and maintain models.\n",
    "\n",
    "Tech stack: ML Algorithms, Python, SQL, Spark, Git, Cloud Services, Deep Learning frameworks, MLOps, etc\n",
    "\n",
    " \n",
    "\n",
    "Natural Language Processing\n",
    "\n",
    "Roles: Design text analytics solutions for business problems; develop, evaluate, optimize, deploy and maintain text processing and analytics solutions.\n",
    "\n",
    "Tech stack: Python, SQL, Git, NLTK, Deep Learning frameworks, MLOps, Text analytics, NLP, NLU, NLG, Language Models, etc\n",
    "\n",
    " \n",
    "\n",
    "Computer Vision\n",
    "\n",
    "Roles: Design Image and video analytics solutions for business problems; develop, evaluate, optimize, deploy and maintain solutions\n",
    "\n",
    "Tech stack: Tensorflow, OpenCV, Fastai, Pytorch, MLFlow, Spark, MLlib Python, SQL, Git, Deep Learning frameworks, MLOps, etc\n",
    "\n",
    " \n",
    "\n",
    "Optimization / Simulation\n",
    "\n",
    "Roles: Design optimization/simulation analytics solutions for business problems; develop, evaluate, optimize, deploy and maintain solutions\n",
    "\n",
    "Tech stack: mathematical/process models, Simulation modeling, AnyLogic, Simio, mixed-integer programming (linear and nonlinear), Python, Pyomo, Gurobi solver, MLOps, etc.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=\"\"\"\n",
    "Develop, design, and implement mechanical solutions including but not limited to enclosures and mechanisms while managing constraints in space, time, cost, manufacturing, structural integrity, and thermal loads.\n",
    "\n",
    "Direct and coordinate mechanical engineering and manufacturing services as required.\n",
    "\n",
    "Develop project documentation including specifications, 3D CAD models, 2D drawings, schedules and presentations to capture and communicate the mechanical design to achieve the defined project goals.\n",
    "\"\"\"\n",
    "\n",
    "job_requirements = \"\"\"\n",
    "BS or MS in Mechanical Engineering with 5 to 15 years of experience in electronic package design/ mechanical enclosure design for hand held products.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDevelop, design, and implement mechanical solutions including but not limited to enclosures and mechanisms while managing constraints in space, time, cost, manufacturing, structural integrity, and thermal loads.\\n\\nDirect and coordinate mechanical engineering and manufacturing services as required.\\n\\nDevelop project documentation including specifications, 3D CAD models, 2D drawings, schedules and presentations to capture and communicate the mechanical design to achieve the defined project goals.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_487/1236233019.py:87: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='b94981b8-3049-4d0f-ab70-c0513d3b01fb', embedding=None, metadata={'page_label': '1', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='238b4768-fdc7-48eb-9327-edf5b0c4677a', embedding=None, metadata={'page_label': '2', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='41705057-101c-45b8-b6a4-8cabae116003', embedding=None, metadata={'page_label': '3', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it. Learn more: https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 119\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m criteria_list:\n\u001b[1;32m    114\u001b[0m     resume_screener \u001b[38;5;241m=\u001b[39m ResumeScreenerPack(\n\u001b[1;32m    115\u001b[0m         job_description\u001b[38;5;241m=\u001b[39mjob_description,\n\u001b[1;32m    116\u001b[0m         job_requirements\u001b[38;5;241m=\u001b[39mjob_requirements,\n\u001b[1;32m    117\u001b[0m         criteria\u001b[38;5;241m=\u001b[39mcriteria,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mresume_screener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume/Ang Teik Hun Resume.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m    121\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(response)\n",
      "Cell \u001b[0;32mIn[6], line 104\u001b[0m, in \u001b[0;36mResumeScreenerPack.run\u001b[0;34m(self, resume_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mload_data(Path(resume_path))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs)\n\u001b[0;32m--> 104\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mNodeWithScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mresponse\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py:184\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    182\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    183\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 184\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    193\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/core/response_synthesizers/tree_summarize.py:175\u001b[0m, in \u001b[0;36mTreeSummarize.get_response\u001b[0;34m(self, query_str, text_chunks, **response_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m             response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    170\u001b[0m                 summary_template,\n\u001b[1;32m    171\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mtext_chunks[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    172\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    173\u001b[0m             )\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m             response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43msummary_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_chunks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# summarize each chunk\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py:218\u001b[0m, in \u001b[0;36mLLM.structured_predict\u001b[0;34m(self, output_cls, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprogram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_program_for_llm\n\u001b[1;32m    211\u001b[0m program \u001b[38;5;241m=\u001b[39m get_program_for_llm(\n\u001b[1;32m    212\u001b[0m     output_cls,\n\u001b[1;32m    213\u001b[0m     prompt,\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    215\u001b[0m     pydantic_program_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_program_mode,\n\u001b[1;32m    216\u001b[0m )\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/program/openai/base.py:172\u001b[0m, in \u001b[0;36mOpenAIPydanticProgram.__call__\u001b[0;34m(self, llm_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m openai_fn_spec \u001b[38;5;241m=\u001b[39m to_openai_tool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls, description\u001b[38;5;241m=\u001b[39mdescription)\n\u001b[1;32m    170\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39mformat_messages(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 172\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mopenai_fn_spec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m message \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:93\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m     85\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     86\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     87\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m         },\n\u001b[1;32m     92\u001b[0m     )\n\u001b[0;32m---> 93\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:237\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:296\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m    295\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 296\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    302\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/fyp_goo/venv/lib/python3.11/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it. Learn more: https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.readers.file import PDFReader\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "\n",
    "\n",
    "# backwards compatibility\n",
    "try:\n",
    "    from llama_index.core.llms.llm import LLM\n",
    "except ImportError:\n",
    "    from llama_index.core.llms.base import LLM\n",
    "\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "[Instruction] You will be provided with details such as the screening criteria, job description and job requirements.\n",
    "Please act as an impartial judge and evaluate the candidate's {criteria_str} based on the job description and job requirements. For this evaluation, you should primarily consider the following accuracy:\n",
    "[Accuracy]\n",
    "Score 1: The candidate's {criteria_str} is completely unrelated to job_description and job_requirements.\n",
    "Score 3: The candidate's {criteria_str} has minor relevance but does not align with job_description and job_requirements.\n",
    "Score 5: The candidate's {criteria_str} has moderate relevance but contains inaccuracies to job_description and job_requirements.\n",
    "Score 7: The candidate's {criteria_str} aligns with job_description and job_requirements. but has minor errors or omissions on either one of them.\n",
    "Score 10: The candidate's {criteria_str} is completely accurate and aligns very well with job_description and job_requirements.\n",
    "        \n",
    "\n",
    "### Job Description\n",
    "{job_description}\n",
    "\n",
    "### Job Requirements\n",
    "{job_requirements}\n",
    "\n",
    "### Screening Criteria\n",
    "{criteria_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CriteriaDecision(BaseModel):\n",
    "    \"\"\"The decision made based on a single criteria.\"\"\"\n",
    "\n",
    "    scoring: int = Field(description=\"The scoring in the scale of 1-10 made based on the criteria\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the decision\")\n",
    "\n",
    "\n",
    "class ResumeScreenerDecision(BaseModel):\n",
    "    \"\"\"The decision made by the resume screener.\"\"\"\n",
    "\n",
    "    criteria_decisions: List[CriteriaDecision] = Field(\n",
    "        description=\"The decisions made based on the criteria\"\n",
    "    )\n",
    "    overall_reasoning: str = Field(\n",
    "        description=\"The reasoning behind the overall decision\"\n",
    "    )\n",
    "    scoring: int = Field(\n",
    "        description=\"The overall scoring in the scale of 1-10 made based on the overall criteria\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _format_criteria_str(criteria: List[str]) -> str:\n",
    "    criteria_str = \"\"\n",
    "    for criterion in criteria:\n",
    "        criteria_str += f\"- {criterion}\\n\"\n",
    "    return criteria_str\n",
    "\n",
    "\n",
    "class ResumeScreenerPack(BaseLlamaPack):\n",
    "    def __init__(\n",
    "        self, job_description: str, job_requirements:str, criteria: str, llm: Optional[LLM] = None\n",
    "    ) -> None:\n",
    "        self.reader = PDFReader()\n",
    "        llm = llm or OpenAI(model=\"gpt-4\")\n",
    "        service_context = ServiceContext.from_defaults(llm=llm)\n",
    "        self.synthesizer = TreeSummarize(\n",
    "            output_cls=ResumeScreenerDecision, service_context=service_context\n",
    "        )\n",
    "        self.query = QUERY_TEMPLATE.format(\n",
    "            job_description=job_description, job_requirements=job_requirements, criteria_str=criteria\n",
    "        )\n",
    "\n",
    "    def get_modules(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get modules.\"\"\"\n",
    "        return {\"reader\": self.reader, \"synthesizer\": self.synthesizer}\n",
    "\n",
    "    def run(self, resume_path: str, *args: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run pack.\"\"\"\n",
    "        docs = self.reader.load_data(Path(resume_path))\n",
    "        print(docs)\n",
    "\n",
    "        output = self.synthesizer.synthesize(\n",
    "            query=self.query,\n",
    "            nodes=[NodeWithScore(node=doc, score=1.0) for doc in docs],\n",
    "        )\n",
    "        return output.response\n",
    "    \n",
    "criteria_list=[\"educational background\", \"skill groups\",\"job experience\"]\n",
    "results = []\n",
    "\n",
    "for criteria in criteria_list:\n",
    "    resume_screener = ResumeScreenerPack(\n",
    "        job_description=job_description,\n",
    "        job_requirements=job_requirements,\n",
    "        criteria=criteria,\n",
    "    )\n",
    "    response = resume_screener.run(resume_path=\"resume/Ang Teik Hun Resume.pdf\")\n",
    "    print(response)\n",
    "    results.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\n",
      "\n",
      "Only extract the properties mentioned in the 'information_extraction' function.\n",
      "\n",
      "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
      "\n",
      "Passage:\n",
      "[Document(page_content=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0}), Document(page_content='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 1}), Document(page_content='', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 2})]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Document(page_content='name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: 1 year\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Data Mining, Relational Database, Deep Learning\\nRelevant experience: Data Scientist with experience in data mining, machine learning, and database management\\n', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "file_path = \"resume/Ang Teik Hun Resume.pdf\"\n",
    "\n",
    "results = extract_information(documents)\n",
    "\n",
    "document_objects = []\n",
    "# Construct page content from dictionary values\n",
    "page_content = \"\"\n",
    "for key, value in results[0].items():\n",
    "    page_content += f\"{key}: {value}\\n\"\n",
    "\n",
    "# Construct metadata\n",
    "metadata = {'source': file_path, 'page': 0}  # You may adjust the source accordingly\n",
    "\n",
    "# Create Document object\n",
    "document = Document(page_content=page_content, metadata=metadata)\n",
    "document_objects.append(document)\n",
    "\n",
    "print(document_objects)\n",
    "# db.add_documents(documents=document_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='2536a754-4ce1-4a78-ae14-3ae5a944f6f0', embedding=None, metadata={'page_label': '1', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b80f35f9-0f17-49fe-8b35-fa16da733327', embedding=None, metadata={'page_label': '2', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6928b86d-c249-40dc-b7ce-86f983461296', embedding=None, metadata={'page_label': '3', 'file_name': '/workspaces/fyp_goo/trash/resume/Ang Teik Hun Resume.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criteria' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mresume_screener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume/Ang Teik Hun Resume.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[40], line 84\u001b[0m, in \u001b[0;36mResumeScreenerPack.run\u001b[0;34m(self, resume_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs)\n\u001b[1;32m     83\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcriteria\u001b[49m:\n\u001b[1;32m     85\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(job_description\u001b[38;5;241m=\u001b[39mjob_description,job_requirements\u001b[38;5;241m=\u001b[39mjob_requirements,criteria \u001b[38;5;241m=\u001b[39m x)\n\u001b[1;32m     86\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criteria' is not defined"
     ]
    }
   ],
   "source": [
    "response = resume_screener.run(resume_path=\"resume/Ang Teik Hun Resume.pdf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='38c156bb-ce4a-422c-b173-bffb89325088', embedding=None, metadata={'page_label': '1', 'file_name': '/workspaces/fyp_goo/trash/resume/GooYeJui_CV.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"SEPT 2023 - JUNE 2024\\nJUNE 2021 - JULY 2021 PRODUCTION OPERATOR\\nJINKO SOLAR TECHNOLOGY SDN BHDDATA SCIENCE INTERN\\nPETRONAS DIGITAL SDN. BHD.\\nAssisted in the development of machine learning model contributing to the automation of critical business processes.\\nCollaborated with a team of data professionals to work on live projects, applying data analytics and machine learning\\nto derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools,\\nand technologies, furthering the journey toward becoming a proficient data scientist.\\nApplied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume\\nParser App.\\nIntegrated a new feature to automate the process of identifying the candidate’s criteria specified by HR.\\nImproved   User Experience for the Resume Parser Flask Application by integrating the email feature.\\nAssisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment,\\ncontributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and\\nadherence to safety protocols.\\nOperate automated machinery and equipment for the production of wafer.\\nMonitor the production process to ensure product quality meets company standards.\\nStrive to maximize production efficiency by minimizing downtime and improving processes.PROFESSIONAL EXPERIENCEA highly motivated and skilled data scientist with a strong background in computer science. Proven experience in using\\na variety of data mining and machine learning techniques to solve complex problems. Proficient in Python, R, and SQL.   \\nAdept at using data science tools and methodologies to deliver valuable solutions and contribute to data-driven\\ndecision-making. Eager to apply my skills and knowledge in a professional data science role to make a positive impact.+6018-4040438 | yjyejui626@gmail.com | linkedin.com/in/goo-ye-jui/\\nLORONG JAMBU MADU 2, TAMAN JAMBU MADU\\n14000 BUKIT MERTAJAM, PENANGGOO YE JUI\\nPROFILE\\nUNIVERSITI TEKNOLOGI MALAYSIA | CGPA : 3.74\\nUNIVERSITI TEKNOLOGI MALAYSIA | CGPA : 3.96Foundation in Science\\nBachelor of Computer Science (Data Engineering) With HonoursEDUCATION\\nEnglish Chinese Malay FrenchLANGUAGES\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3d55da98-edde-4e24-bf4d-5c8673bef075', embedding=None, metadata={'page_label': '2', 'file_name': '/workspaces/fyp_goo/trash/resume/GooYeJui_CV.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='MAR 2023 - PRESENT\\nOCT 2020 -  PRESENTMAYBANK STUDENT AMBASSADOR\\nMEMEBR OF PERSATUAN MAHASISWA SAINS KOMPUTEROTHER ROLES\\nPython (NumPy, Pandas,\\nscikit-learn, spaCy, Keras,\\nNLTK, matplotlib)SQL\\nAWS\\nAlteryxC++\\nC# (ASP .NET)\\nPower BI/ TableauNoSQL (MongoDB)\\nMachine Learning\\nArtificial Intelligence (AI)TECHNICAL SKILLSCOMMITEE MEMBER & PARTICIPANT\\nSTARTUP WEEKEND JOHOR BAHRU SWJB@UTM 2020SEPT 2021\\nGained an opportunity to brainstorm for something which could possibly become the next big thing in\\nentrepreneurship.\\nExposed to the concept and creative idea in solving problems as well as cultivating cooperation among the\\nparticipants through several activities.\\nLearnt many business-related concepts such as ‘Business Model Canvas’, ‘Validation Board’ and ‘Business\\nPitching’.FIRST RUNNER-UP | TEAM NAUTILUS UNIVERSITI TEKNOLOGI MALAYSIA \\nMonsoonSIM Enterprise Resource Management Competition (MERMC) SEPT 2022\\nRan several virtual companies collaboratively (with different departments like Finance, Retailing, B2B, Marketing,\\nE-Commerce, Manufacturing, Human Resource, Logistics, R&D, etc.)  \\nExposed to the concept of enterprise resource management (ERM) through fun and exciting simulation games and\\ncompetition.\\nImproved   User Experience for the Resume Parser Flask Application by integrating the email feature.EXTRA-CURRICULAR ACTIVITIES AND AWARDS\\nPARTICIPANT\\nA DATATHON EVENT: THE SUSTAINABLE FOOD NEXUSAUG 2023\\nGained an opportunity to brainstorm for exploration, innovation, and problem-solving, fueled by the transformative\\npower of data.\\nExposed to the well-being of our planet through the lens of data science and analytics.\\nEnhanced my data analytics skill as I tackle pressing global issues and derive insightful solutions with the support of\\nresources and guidance from passionate mentors.\\nCERTIFICATIONS\\nAWS Academy Graduate - AWS Academy Cloud Foundations | Nov 2022\\nAWS Academy Graduate - AWS Academy Machine Learning Foundations | Dec 2022\\nAWS Academy Graduate - AWS Academy Data Analytics | Dec 2022\\nAWS Academy Graduate - AWS Academy Machine Learning for Natural Language Processing | June 2023\\nAWS Academy Graduate - AWS Academy Data Engineering | June 2023\\nAWS Academy Graduate - AWS Academy Cloud Web Application Builder | June 2023\\nAWS Academy Graduate - AWS Academy Cloud Data Pipeline Builder | June 2023AWS Academy Graduate Certificates\\nAlteryx Foundational Micro-Credential | Dec 2022\\nAlteryx Designer Core Certification | Dec 2022Alteryx Certificates\\nOthers\\nMicrosoft Certified: Azure AI Fundamentals | Jan 2023\\nGoogle Data Analytics Certificate by Coursera | August 2023\\nIBM Machine Learning Professional Certificate by Coursera | October 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "criteria_decisions=[CriteriaDecision(scoring=8, reasoning='Strong educational background with a Bachelor of Computer Science (Data Engineering) with Honours from Universiti Teknologi Malaysia.'), CriteriaDecision(scoring=9, reasoning='Proficient in a variety of technical skills including Python, SQL, machine learning algorithms, natural language processing, and deep learning frameworks.')] overall_reasoning='The candidate meets the screening criteria with a strong educational background and a diverse skill set that aligns well with the job description and requirements.' scoring=9\n"
     ]
    }
   ],
   "source": [
    "response = resume_screener.run(resume_path=\"resume/GooYeJui_CV.pdf\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
