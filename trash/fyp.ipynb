{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from uuid import uuid4\n",
    "load_dotenv(\".env\")\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "serp_api_key = os.environ.get('SERPER_API_KEY')\n",
    "\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/fyp_goo/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/workspaces/fyp_goo/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\n",
      "\n",
      "Only extract the properties mentioned in the 'information_extraction' function.\n",
      "\n",
      "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
      "\n",
      "Passage:\n",
      "[Document(page_content=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0}), Document(page_content='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 1}), Document(page_content='', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 2})]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Document(page_content='name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: 1 year\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Data Mining, Relational Database, Deep Learning\\nRelevant experience: Data Scientist with experience in data mining, machine learning, and database management\\n', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bf6acb02-d5e7-11ee-aec6-6045bd1ea272']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from dataclasses import dataclass\n",
    "chroma_directory = 'db/'\n",
    "\n",
    "db = Chroma(persist_directory=chroma_directory, embedding_function=OpenAIEmbeddings())\n",
    "#file_path = \"JiaHao_Lo.pdf\"\n",
    "#file_path = \"GooYeJui_CV.pdf\"\n",
    "file_path = \"resume/Ang Teik Hun Resume.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "results = extract_information(documents)\n",
    "\n",
    "document_objects = []\n",
    "# Construct page content from dictionary values\n",
    "page_content = \"\"\n",
    "for key, value in results[0].items():\n",
    "    page_content += f\"{key}: {value}\\n\"\n",
    "\n",
    "# Construct metadata\n",
    "metadata = {'source': file_path, 'page': 0}  # You may adjust the source accordingly\n",
    "\n",
    "# Create Document object\n",
    "document = Document(page_content=page_content, metadata=metadata)\n",
    "document_objects.append(document)\n",
    "\n",
    "print(document_objects)\n",
    "db.add_documents(documents=document_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' \\nCurriculum Vitae/Resume  \\nName: Ang Teik Hun  \\n \\nPROFILE  \\nA highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. \\nProven experience in using a variety of data mining and machine learning techniques to solve complex problems. \\nProficient in Python, R, and  SQL.  \\n \\n EXPERIENCE  \\nInternship as Data Scientist, Petroliam Nasional Berhad  \\nGroup Digital , KL / November 2020  – July 2021  \\n• Used regular expression and other text pre -processing features in python \\nto automate the extraction of information  from corporate business \\ndatabase , minimizing data quality issues by 90% . \\n• Utilized Object Orient ed Programming (class and subclass) for the \\nconversion of data from JSON to panda, performed queries such as \\nappend, split and find  all to categorize the databa se. \\n• Applied techniques like natural processing language processing and text \\nmining on corporate business database (Fuzzy mapping), increased \\nmapping accuracy by 80%.  \\n• Performed feature selection using filter, wrapper and embedded method \\nto identify the sign ificant predictors for machine learning model, compared \\nthe results with key influencer analysis from Power B I. \\n• Forecasted the profit ability of oil and gas investment by  generating  linear \\nregression model based on the  significant predictors , provided data \\nvisualization using matplotlib , seaborn  and Power B I. \\n• Assist ed in unsupervised  clustering  model training  using K mean and \\nhierarchical  algorithms  for oil and gas offshore platform resource \\ndevelopment . LINKS  \\nAustralian National University , COMP6240 Relational Databases  \\nTutor  \\nAustralian National University  / June  2022 – Dec 2022  \\n• Teach courses on database management and design, including the use \\nof SQL to define, query, and manipulate a relational database . \\n• Introduce database design methods based on functional dependencies \\nand normal forms for evaluating the quality of a database design . \\n• Cover topics such as query processing and optimization, transaction \\nmanagement, and security management in a relational database \\nmanagement system . \\n \\nEDUCATION  \\nMaster of Applied Data Analytics , Australian National University , \\nCanberra, Australia   \\nJuly 2021  – Dec 2022  \\n• Major in Neural Network and Document Analysis  \\n• CGPA 6.42/7  cumulative.  \\n• Weight Averaged Mark, 80/100 . \\n• Relevant courseworks: Information Retrieval, Data Mining, Relational \\nDatabase, Deep Learning and etc.  \\n https://www.linkedin.com/in/\\nteik-hun-ang-60233a1b9/  \\nSKILLS  \\n Analytical Thinking Skills  \\n Adaptability  \\n Time Management  \\n Leadership  \\n Power BI  \\n Python  \\n Neural Network  \\n Machine Learning (S Klearn)  \\n Database SQL  \\n Data Wrangling  \\n Optimization  \\nCERTIFICATION  \\nMicrosoft Certified: Azure \\nFundamentals  \\nNov 2020  \\nMicrosoft Certified: Azure AI \\nEngineer Associate  \\nOct 2020 – Oct 2021  \\n \\nMEMBERSHIP  \\nInstitution of Civil Engineers, \\nICE Board of Engineer Malaysia, \\nGraduate Engineer   \\n \\n28, Lorong Pandan Indah 1, Taman \\nPandan Indah, 13400 Butterworth, \\nPenang, Malaysia.  \\nPhone: +60124773683  \\nEmail address: teikhun0422@hotmail.com  \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 0}),\n",
       " Document(page_content='HOBBIES  \\nADVANCED COURSES  Gym  and Cooking  \\n \\nData & AI Essentials, IBM  \\nhttps://www.youracclaim.com/badges/6a6700f0 -a3ef -4b0a -\\n8fc2 3475c0897dd6?source=linked_in_profile  \\n \\nMachine Learning A -Z™: Hands -On Python & R In Data Science, Udemy  \\nhttps://www.udemy.com/certificate/UC -2fee7097 -3f3f-430a -ab8a -3196e8f5be89/   \\n \\n70-778, DA -100: Analyzing and Visualizing Data with Power BI, Udemy  \\nhttps://www.udemy.com/certificate/UC -7b5b1dd2 -3225 -46bc -a731 -58adb9faa07c/  \\n \\nMaster statistics & machine learning: intuition, math, code, Udemy  \\nhttps://www.udemy.com/certificate/UC -cbe8daad -70a6 -4c83 -bac8 -eba86c90ae84/  \\n \\nData Science Math Skills, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/AFA7VK58ZXVK  \\n \\nComputer Vision Basic, Coursera  \\nhttps://www.coursera.org/account/accomplishments/certificate/GN2ZGC9GMTNJ  \\n \\nRelevant coursework:  \\nDescriptive and inferential statistics , T-tests, correlation, ANOVA, Linear Regression , \\nClassification, Clustering, Reinforcement Learning, Natural Language Processing, \\nDeep Learning, Dimensionality Reduction , Model Selection and Boosting.  LANGUAGES  \\nEXTRA -CURRICULAR ACTIVITIES AND AWARDS  \\nPUBLICATION  \\nChinese  \\nEnglish  \\nMalay  \\nFrench  \\nREFERENCES  \\nInsight on Extraction and \\nCharacteri zation of \\nBiopolymers as the Green \\nCoagulants for Microalgae \\nHarvesting  \\nQ2, MDPI Water, M ay 2020  \\nhttps://www.mdpi.com/2073 -\\n4441/12/5/1388  \\n \\nEncyclopedia, Sept 2020  \\nhttps://encyclopedia.pub/1462  \\nA/Prof Qing Wang  \\nSchool of Computing  \\nThe Australian National \\nUniversity Canberra, ACT 0200  \\n+61(0)261254625  \\nqing.wang@anu.edu.au  \\n \\n \\nDr Vikram Sorathia  \\nHead  (Data Science) at \\nPETRONAS  \\n+(60)18-9826762   \\nvikrambhai.sorathia@petronas\\n.com  ANU+ Award   \\nJuly 2021  – Dec 2022  \\n• The ANU+ Award is a program that formally recognizes  experience and \\ncontribution achieved through volunteering . Volunteer 40 hours in \\ncomputing related field.  \\n \\nCLUBS & SPORT  \\n• Head of Ceremony and Protocol department, Medical Camp , Rotaract Club.  \\n• Assistance Head of Club Serve Department, Rotaract Club.  \\n \\n \\n \\n \\n ', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 1}),\n",
       " Document(page_content='', metadata={'source': 'resume/Ang Teik Hun Resume.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the names of these candidates?',\n",
       " 'chat_history': [SystemMessage(content='')],\n",
       " 'answer': \"I'm sorry, but I don't have any information about candidates. If you provide me with some details or context, I can try to help you further.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def get_doc_search(text_splitter):\n",
    "    return FAISS.from_texts(text_splitter, OpenAIEmbeddings())\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "db_retriever = db.as_retriever()\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=db_retriever, memory=memory)\n",
    "qa(\"What are the names of these candidates?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is Ang Teik Hun?',\n",
       " 'chat_history': [SystemMessage(content='The human asks for the names of the candidates. The AI apologizes and explains that the information provided does not include the names of the candidates.')],\n",
       " 'answer': 'Ang Teik Hun is a highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. They have experience in using data mining and machine learning techniques to solve complex problems. Ang Teik Hun is proficient in Python, R, and SQL. They have completed a Master of Applied Data Analytics at the Australian National University, majoring in Neural Network and Document Analysis. Additionally, they have certifications in Microsoft Azure Fundamentals and Azure AI Engineer Associate. Ang Teik Hun has also worked as a Data Scientist Intern at Petroliam Nasional Berhad and is currently a tutor at the Australian National University teaching courses on database management and design.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"Who is Ang Teik Hun?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import Tool\n",
    "import random\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "\n",
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "# def text_extractor(file_path=\"\"):\n",
    "#     file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "#     pdf_reader = PdfReader(f\"{file_path}\")\n",
    "\n",
    "#     # read data from the file and put them into a variable called text\n",
    "#     text = ''\n",
    "#     for i, page in enumerate(pdf_reader.pages):\n",
    "#         extracts = page.extract_text()\n",
    "#         text += extracts\n",
    "#     print(\"final result:\",text)\n",
    "#     return text\n",
    "\n",
    "def text_extractor(file_path=\"\"):\n",
    "    file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "    # Retrieve and generate using the relevant snippets of the blog.\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "text_extractor_tool = Tool(\n",
    "                        name = \"text_extractor\",\n",
    "                        func=text_extractor,\n",
    "                        description=\"useful for when you need to extract text from a pdf of the resume.\"\n",
    "                    )\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "\n",
    "information_extractor_tool = Tool(\n",
    "                        name = \"extract_information\",\n",
    "                        func=extract_information,\n",
    "                        description=\"useful for when you need to extract information into structured python dictonary.\"\n",
    "                    )\n",
    "\n",
    "def educational_background(input):\n",
    "    return input[0]['Education Background']\n",
    "\n",
    "educational_background_tool = Tool(\n",
    "                        name = \"educational_background\",\n",
    "                        func=educational_background,\n",
    "                        description=\"useful for when you need to answer question about education background.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"educational_background\",\n",
      "    \"action_input\": \"Which university is this candidate from?\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 24\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# create our agent\u001b[39;00m\n\u001b[0;32m     14\u001b[0m conversational_agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[0;32m     15\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat-conversational-react-description\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m \u001b[43mconversational_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhich university is this candidate from?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1391\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1391\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1400\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1401\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1097\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1097\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1097\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1097\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1182\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1204\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[1;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m-> 1204\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    423\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    379\u001b[0m     )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:584\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[0;32m    576\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[0;32m    579\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    580\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    582\u001b[0m         )\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m--> 584\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     )\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 72\u001b[0m, in \u001b[0;36meducational_background\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meducational_background\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEducation Background\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = [text_extractor_tool, information_extractor_tool, educational_background_tool]\n",
    "\n",
    "# conversational agent memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=3,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# create our agent\n",
    "conversational_agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=turbo_llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "conversational_agent(\"Which university is this candidate from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate, Ang Teik Hun, graduated with a Master of Applied Data Analytics from the Australian National University in Canberra, Australia in December 2022.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = PyPDFLoader('Ang Teik Hun Resume.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import retriever\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "chain = RetrievalQA.from_chain_type(llm=turbo_llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query=\"When was this candidate graduated?\"\n",
    "out = chain.invoke(query)\n",
    "print(out['result'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "import random\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "\n",
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "def text_extractor(file_path=\"\"):\n",
    "    file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "    pdf_reader = PdfReader(f\"{file_path}\")\n",
    "\n",
    "    # read data from the file and put them into a variable called text\n",
    "    text = ''\n",
    "    for i, page in enumerate(pdf_reader.pages):\n",
    "        extracts = page.extract_text()\n",
    "        text += extracts\n",
    "    print(\"final result:\",text)\n",
    "    return text\n",
    "\n",
    "text_extractor_tool = Tool(\n",
    "                        name = \"text_extractor\",\n",
    "                        func=text_extractor,\n",
    "                        description=\"useful for when you need to extract text from a pdf of the resume.\"\n",
    "                    )\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "\n",
    "information_extractor_tool = Tool(\n",
    "                        name = \"extract_information\",\n",
    "                        func=extract_information,\n",
    "                        description=\"useful for when you need to extract information into structured python dictonary.\"\n",
    "                    )\n",
    "\n",
    "def educational_background(input):\n",
    "    return input[0]['Education Background']\n",
    "\n",
    "educational_background_tool = Tool(\n",
    "                        name = \"educational_background\",\n",
    "                        func=educational_background,\n",
    "                        description=\"useful for when you need to answer question about education background.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents to plan and execute tasks more efficiently by dividing them into manageable components. Task decomposition can be achieved through various methods such as prompting with specific instructions or utilizing human inputs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in this context?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition can be done in common ways such as using prompting techniques like Chain of Thought or Tree of Thoughts to guide models to break down tasks into smaller steps. Additionally, task-specific instructions can be provided to help with decomposition, such as asking for subgoals or outlining steps for achieving a specific task. Human inputs can also be utilized for task decomposition, allowing for a more tailored approach to breaking down complex tasks.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m CONDENSE_QUESTION_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(_template)\n\u001b[1;32m     19\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnswer the question based only on the following context:\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m ANSWER_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_template(template)\n\u001b[1;32m     27\u001b[0m DEFAULT_DOCUMENT_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{page_content}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_combine_documents\u001b[39m(\n\u001b[1;32m     31\u001b[0m     docs, document_prompt\u001b[38;5;241m=\u001b[39mDEFAULT_DOCUMENT_PROMPT, document_separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "def text_extractor():\n",
    "    chroma_directory = 'db3/'\n",
    "\n",
    "    db = Chroma(persist_directory=chroma_directory, embedding_function=OpenAIEmbeddings())\n",
    "    db_retriever = db.as_retriever()\n",
    "\n",
    "    return db_retriever\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | text_extractor(),\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='1. Goo Ye Jui\\n2. Jia-Hao Lo\\n3. Ang Teik Hun'),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'})]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Name these candidates one by one.\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Name these candidates one by one.'),\n",
       "  AIMessage(content='1. Goo Ye Jui\\n2. Jia-Hao Lo\\n3. Ang Teik Hun'),\n",
       "  HumanMessage(content='In the scale of 1-10, how would you rate their skill sets as a Data Scientis?'),\n",
       "  AIMessage(content='Based on the information provided, Goo Ye Jui has relevant experience as a Data Science Intern and has a Bachelor of Computer Science in Data Engineering with Honours. Jia-Hao Lo has a Master of Science in Analytics and relevant experience in Data Science Department On-Job Training. Ang Teik Hun has a Master of Applied Data Analytics with a background in Neural Network and Document Analysis. \\n\\nConsidering their education, experience, and skills, Goo Ye Jui would be rated around 7, Jia-Hao Lo around 6, and Ang Teik Hun around 8 as Data Scientists on a scale of 1-10.')]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content=\"Based on the provided information, Goo Ye Jui seems to have a solid foundation in data science with relevant experience in developing machine learning models and working on live projects. Jia-Hao Lo has experience in data science through on-job training and engineering internships. Ang Teik Hun has a strong background in data science with a Master's degree in Applied Data Analytics and relevant experience in using various data mining and machine learning techniques.\\n\\nOn a scale of 1-10, Goo Ye Jui could be rated around 7, Jia-Hao Lo around 5, and Ang Teik Hun around 8 based on their education level, relevant experience, and skills mentioned in the context.\"),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'})]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"In the scale of 1-10, how would you rate their skill sets as a Data Scientis?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content=\"Based on the provided information, the candidates can be ranked based on their skill set score as follows:\\n\\n1. Ang Teik Hun\\n2. Goo Ye Jui\\n3. Jia-Hao Lo\\n\\nAng Teik Hun has a comprehensive skill set that includes a strong background in data analytics, machine learning, neural networks, and various relevant certifications and courses. Goo Ye Jui also has relevant experience in data science and machine learning, but Ang Teik Hun's skill set appears to be more extensive. Jia-Hao Lo has a background in analytics and engineering but may not have as much specific experience in data science and machine learning as the other two candidates.\"),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'})]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Please rank the candidates based on their skill set score.\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'EMAIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dotenv_values\n\u001b[1;32m      3\u001b[0m secrets \u001b[38;5;241m=\u001b[39m dotenv_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf.env\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m hf_email \u001b[38;5;241m=\u001b[39m \u001b[43msecrets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEMAIL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m hf_pass \u001b[38;5;241m=\u001b[39m secrets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPASS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EMAIL'"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "secrets = dotenv_values('hf.env')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
