{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "from uuid import uuid4\n",
    "load_dotenv(\".env\")\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "serp_api_key = os.environ.get('SERPER_API_KEY')\n",
    "\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\n",
      "\n",
      "Only extract the properties mentioned in the 'information_extraction' function.\n",
      "\n",
      "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
      "\n",
      "Passage:\n",
      "[Document(page_content='Jia-Hao Lo   \\njiahaolo@u.northwestern.edu | www.linkedin.com/in/jiahaolo | (872) -810-5588  \\nEDUCATION \\n \\nNorthwestern University CGPA: 3. 83/4.00                                                                                        Evanston, Illinois  \\nMaster of Science in Analytics                                                                                     Sept 2021 – Dec 202 2 (Expected)  \\nPETRONAS Educational Sponsorship Program 2015 ( Full sponsorship for postgraduate study) \\n \\nUniversity Technology P ETRONAS  CGPA: 3.92/4. 00                                                                   Malaysia   \\nBachelor of Civil Engineering  in Offshore Engineering                                      Sept 2016 – Sept 2020  \\nFirst Class Honors , Dean’s List Award (2016 -2020)  \\nPETRONAS  Educational Sponsorship Program 2015 ( Top 3% of >10000 applicants)   \\n \\nRelevant & Expected Coursework :  \\nPredictive  Analytics, Databases (SQL) , Data Visualization  (D3.js) , Data Mining, Analytics Value Chain  (A/B Test, \\nAWS, Docker, Flask) , Analytics of Big Data  (Spark, Hadoop, Hive, MapReduce) , Deep Learning, Text Analytics  \\n \\nRelevant Certification s: \\nAzure AI Engineer Associate, Azure Fundamentals , IBM Data Science Professional Certificate  \\n \\nWORK EXPERIENCE \\n \\nPetroliam Nasional Berhad (PETRONAS)                                           Malaysia  \\nData Science Department On-Job Training                                                       Nov 2020 – July 2021  \\n• Delivered ex ploratory data analysis,  hypothesis testing and cluster ing analysis to perform customer segmentation \\nof two depots  into three tiers  based on customer lifetime value.   \\n• Used superv ised learning model in Python to i dentif y and predict upsell and churned customers of  1000+  petrol \\nstations , models achieved 90% recall.  \\n• Designed  multiple descriptive analytics , data visualizations  and dashboards  using Power BI  based on \\nbenchmarking  effort s of third -party vendor  to provide meaningful insights for five departments.  \\n \\nWeb Structures  Engineering Consultanc y                                               Malaysia  \\nEngineering Internship                                            May 2019 – Dec 201 9 \\n• Performed data cleaning and wrangling for five high-rise construction projects . Produced feasible and quality \\nstructural design on time  using ETABS, SAFE and Microsoft Excel . \\n• Successfully  illustrated  more than 10  proposed solutions and  distilled  technical terms down to publicly accessible \\nlanguage  to clients  and contractors  from different background s to wrap up technical and business meetings.  \\n \\nPROJECT EXPERIENCE \\n \\nCredit Risk Analysis Industry Practicum                      Evanston, Illinois                                                    \\nNorthwestern University                                                                                                  Sept 2021 – June 2022 (Expected)                          \\n• Executing data cleansing, analysis of credit patterns across different branches and algorithm identification based \\non 30 million rows of customers’ application and behavioral data.  \\n• Utilizing machine learning model (logistic regress ion, gradient boosted trees) to predict customers’ probability to \\ndefault and their credit limits to prevent credit defaults and maximize profit margins.  \\n \\nReal Estate Metrics Prediction with Labor Market Data                                                                    Evanston, Illinois                                                    \\nNorthwestern University                                                                                                          Sept 2021 – December  2021                      \\n• Imported more than 200 GB of labor market data into PostgresSQL Database , carried out data preprocessing, \\nexploratory data analysis and data visualization of labor market using SQL and Python.  \\n• Predict ed real estate metrics including median  listing price for selected cities based on local job market  and \\nachieved 0.8 R2 with baseline model.  \\n \\nSKILLS  \\n \\nTechnical Skills:  Python  (NumPy, Pandas, Scikit -Learn, XGBoost, LightGBM) , R (dplyr, ggplot2) , SQL,  \\nGit, Microsoft Azure, Microsoft Power BI , Tableau  \\nLanguage :   Writing and oral proficiency in English, Malay and Mandarin  ', metadata={'source': 'JiaHao_Lo.pdf', 'page': 0})]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'source': 'JiaHao_Lo.pdf', 'page': 0})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['703a3752-d410-11ee-a600-40b0765f3723']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from dataclasses import dataclass\n",
    "chroma_directory = 'db3/'\n",
    "\n",
    "db = Chroma(persist_directory=chroma_directory, embedding_function=OpenAIEmbeddings())\n",
    "file_path = \"JiaHao_Lo.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "results = extract_information(documents)\n",
    "\n",
    "document_objects = []\n",
    "# Construct page content from dictionary values\n",
    "page_content = \"\"\n",
    "for key, value in results[0].items():\n",
    "    page_content += f\"{key}: {value}\\n\"\n",
    "\n",
    "# Construct metadata\n",
    "metadata = {'source': file_path, 'page': 0}  # You may adjust the source accordingly\n",
    "\n",
    "# Create Document object\n",
    "document = Document(page_content=page_content, metadata=metadata)\n",
    "document_objects.append(document)\n",
    "\n",
    "print(document_objects)\n",
    "db.add_documents(documents=document_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efc1cc25-d40d-11ee-a289-40b0765f3723',\n",
       " 'efc1cc26-d40d-11ee-9dc2-40b0765f3723']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"GooYeJui_CV.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "db.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['07c8c641-d40f-11ee-a10e-40b0765f3723']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"JiaHao_Lo.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "db.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Jia-Hao Lo   \\njiahaolo@u.northwestern.edu | www.linkedin.com/in/jiahaolo | (872) -810-5588  \\nEDUCATION \\n \\nNorthwestern University CGPA: 3. 83/4.00                                                                                        Evanston, Illinois  \\nMaster of Science in Analytics                                                                                     Sept 2021 – Dec 202 2 (Expected)  \\nPETRONAS Educational Sponsorship Program 2015 ( Full sponsorship for postgraduate study) \\n \\nUniversity Technology P ETRONAS  CGPA: 3.92/4. 00                                                                   Malaysia   \\nBachelor of Civil Engineering  in Offshore Engineering                                      Sept 2016 – Sept 2020  \\nFirst Class Honors , Dean’s List Award (2016 -2020)  \\nPETRONAS  Educational Sponsorship Program 2015 ( Top 3% of >10000 applicants)   \\n \\nRelevant & Expected Coursework :  \\nPredictive  Analytics, Databases (SQL) , Data Visualization  (D3.js) , Data Mining, Analytics Value Chain  (A/B Test, \\nAWS, Docker, Flask) , Analytics of Big Data  (Spark, Hadoop, Hive, MapReduce) , Deep Learning, Text Analytics  \\n \\nRelevant Certification s: \\nAzure AI Engineer Associate, Azure Fundamentals , IBM Data Science Professional Certificate  \\n \\nWORK EXPERIENCE \\n \\nPetroliam Nasional Berhad (PETRONAS)                                           Malaysia  \\nData Science Department On-Job Training                                                       Nov 2020 – July 2021  \\n• Delivered ex ploratory data analysis,  hypothesis testing and cluster ing analysis to perform customer segmentation \\nof two depots  into three tiers  based on customer lifetime value.   \\n• Used superv ised learning model in Python to i dentif y and predict upsell and churned customers of  1000+  petrol \\nstations , models achieved 90% recall.  \\n• Designed  multiple descriptive analytics , data visualizations  and dashboards  using Power BI  based on \\nbenchmarking  effort s of third -party vendor  to provide meaningful insights for five departments.  \\n \\nWeb Structures  Engineering Consultanc y                                               Malaysia  \\nEngineering Internship                                            May 2019 – Dec 201 9 \\n• Performed data cleaning and wrangling for five high-rise construction projects . Produced feasible and quality \\nstructural design on time  using ETABS, SAFE and Microsoft Excel . \\n• Successfully  illustrated  more than 10  proposed solutions and  distilled  technical terms down to publicly accessible \\nlanguage  to clients  and contractors  from different background s to wrap up technical and business meetings.  \\n \\nPROJECT EXPERIENCE \\n \\nCredit Risk Analysis Industry Practicum                      Evanston, Illinois                                                    \\nNorthwestern University                                                                                                  Sept 2021 – June 2022 (Expected)                          \\n• Executing data cleansing, analysis of credit patterns across different branches and algorithm identification based \\non 30 million rows of customers’ application and behavioral data.  \\n• Utilizing machine learning model (logistic regress ion, gradient boosted trees) to predict customers’ probability to \\ndefault and their credit limits to prevent credit defaults and maximize profit margins.  \\n \\nReal Estate Metrics Prediction with Labor Market Data                                                                    Evanston, Illinois                                                    \\nNorthwestern University                                                                                                          Sept 2021 – December  2021                      \\n• Imported more than 200 GB of labor market data into PostgresSQL Database , carried out data preprocessing, \\nexploratory data analysis and data visualization of labor market using SQL and Python.  \\n• Predict ed real estate metrics including median  listing price for selected cities based on local job market  and \\nachieved 0.8 R2 with baseline model.  \\n \\nSKILLS  \\n \\nTechnical Skills:  Python  (NumPy, Pandas, Scikit -Learn, XGBoost, LightGBM) , R (dplyr, ggplot2) , SQL,  \\nGit, Microsoft Azure, Microsoft Power BI , Tableau  \\nLanguage :   Writing and oral proficiency in English, Malay and Mandarin  ', metadata={'source': 'JiaHao_Lo.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the names of these candidates?',\n",
       " 'chat_history': [SystemMessage(content='')],\n",
       " 'answer': \"I'm sorry, but the information provided does not include the names of the candidates.\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def get_doc_search(text_splitter):\n",
    "    return FAISS.from_texts(text_splitter, OpenAIEmbeddings())\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "db_retriever = db.as_retriever()\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=db_retriever, memory=memory)\n",
    "qa(\"What are the names of these candidates?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is Ang Teik Hun?',\n",
       " 'chat_history': [SystemMessage(content='The human asks for the names of the candidates. The AI apologizes and explains that the information provided does not include the names of the candidates.')],\n",
       " 'answer': 'Ang Teik Hun is a highly motivated and skilled data scientist with a strong background in computer science and statistical analysis. They have experience in using data mining and machine learning techniques to solve complex problems. Ang Teik Hun is proficient in Python, R, and SQL. They have completed a Master of Applied Data Analytics at the Australian National University, majoring in Neural Network and Document Analysis. Additionally, they have certifications in Microsoft Azure Fundamentals and Azure AI Engineer Associate. Ang Teik Hun has also worked as a Data Scientist Intern at Petroliam Nasional Berhad and is currently a tutor at the Australian National University teaching courses on database management and design.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"Who is Ang Teik Hun?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import Tool\n",
    "import random\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "\n",
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "# def text_extractor(file_path=\"\"):\n",
    "#     file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "#     pdf_reader = PdfReader(f\"{file_path}\")\n",
    "\n",
    "#     # read data from the file and put them into a variable called text\n",
    "#     text = ''\n",
    "#     for i, page in enumerate(pdf_reader.pages):\n",
    "#         extracts = page.extract_text()\n",
    "#         text += extracts\n",
    "#     print(\"final result:\",text)\n",
    "#     return text\n",
    "\n",
    "def text_extractor(file_path=\"\"):\n",
    "    file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "    # Retrieve and generate using the relevant snippets of the blog.\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "text_extractor_tool = Tool(\n",
    "                        name = \"text_extractor\",\n",
    "                        func=text_extractor,\n",
    "                        description=\"useful for when you need to extract text from a pdf of the resume.\"\n",
    "                    )\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "\n",
    "information_extractor_tool = Tool(\n",
    "                        name = \"extract_information\",\n",
    "                        func=extract_information,\n",
    "                        description=\"useful for when you need to extract information into structured python dictonary.\"\n",
    "                    )\n",
    "\n",
    "def educational_background(input):\n",
    "    return input[0]['Education Background']\n",
    "\n",
    "educational_background_tool = Tool(\n",
    "                        name = \"educational_background\",\n",
    "                        func=educational_background,\n",
    "                        description=\"useful for when you need to answer question about education background.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"educational_background\",\n",
      "    \"action_input\": \"Which university is this candidate from?\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 24\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# create our agent\u001b[39;00m\n\u001b[0;32m     14\u001b[0m conversational_agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[0;32m     15\u001b[0m     agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat-conversational-react-description\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m \u001b[43mconversational_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhich university is this candidate from?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1391\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1391\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1400\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1401\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1097\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1097\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1097\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1097\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1182\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain\\agents\\agent.py:1204\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[1;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m-> 1204\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    423\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    379\u001b[0m     )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\fyp\\langchain\\Lib\\site-packages\\langchain_core\\tools.py:584\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[0;32m    576\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[0;32m    579\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    580\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    582\u001b[0m         )\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m--> 584\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     )\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 72\u001b[0m, in \u001b[0;36meducational_background\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meducational_background\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEducation Background\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = [text_extractor_tool, information_extractor_tool, educational_background_tool]\n",
    "\n",
    "# conversational agent memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=3,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "# create our agent\n",
    "conversational_agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=turbo_llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "conversational_agent(\"Which university is this candidate from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate, Ang Teik Hun, graduated with a Master of Applied Data Analytics from the Australian National University in Canberra, Australia in December 2022.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = PyPDFLoader('Ang Teik Hun Resume.pdf')\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import retriever\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "chain = RetrievalQA.from_chain_type(llm=turbo_llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query=\"When was this candidate graduated?\"\n",
    "out = chain.invoke(query)\n",
    "print(out['result'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "import random\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "\n",
    "# Set up the turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.3,\n",
    "    model_name='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "def text_extractor(file_path=\"\"):\n",
    "    file_path = \"Ang Teik Hun Resume.pdf\"\n",
    "    pdf_reader = PdfReader(f\"{file_path}\")\n",
    "\n",
    "    # read data from the file and put them into a variable called text\n",
    "    text = ''\n",
    "    for i, page in enumerate(pdf_reader.pages):\n",
    "        extracts = page.extract_text()\n",
    "        text += extracts\n",
    "    print(\"final result:\",text)\n",
    "    return text\n",
    "\n",
    "text_extractor_tool = Tool(\n",
    "                        name = \"text_extractor\",\n",
    "                        func=text_extractor,\n",
    "                        description=\"useful for when you need to extract text from a pdf of the resume.\"\n",
    "                    )\n",
    "\n",
    "def extract_information(documents):\n",
    "    # Schema\n",
    "    schema = {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"phone_number\": {\"type\": \"string\"},\n",
    "            \"email\": {\"type\": \"string\"},\n",
    "            \"local\": {\"type\": \"string\"},\n",
    "            \"last role\": {\"type\": \"string\"},\n",
    "            \"years of experience\": {\"type\": \"string\"},\n",
    "            \"education level\": {\"type\": \"string\"},\n",
    "            \"CGPA\": {\"type\": \"integer\"},\n",
    "            \"University\": {\"type\": \"string\"},\n",
    "            \"Education Background\": {\"type\": \"string\"},\n",
    "            \"Data Science Background\": {\"type\": \"string\"},\n",
    "            \"Relevant experience\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"height\"],\n",
    "    }\n",
    "\n",
    "    # Input\n",
    "    inp = documents\n",
    "\n",
    "    # Run chain\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",verbose=True)\n",
    "    chain = create_extraction_chain(schema, llm,verbose=True)\n",
    "    result = chain.run(inp)\n",
    "    return result\n",
    "\n",
    "\n",
    "information_extractor_tool = Tool(\n",
    "                        name = \"extract_information\",\n",
    "                        func=extract_information,\n",
    "                        description=\"useful for when you need to extract information into structured python dictonary.\"\n",
    "                    )\n",
    "\n",
    "def educational_background(input):\n",
    "    return input[0]['Education Background']\n",
    "\n",
    "educational_background_tool = Tool(\n",
    "                        name = \"educational_background\",\n",
    "                        func=educational_background,\n",
    "                        description=\"useful for when you need to answer question about education background.\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps agents to plan and execute tasks more efficiently by dividing them into manageable components. Task decomposition can be achieved through various methods such as prompting with specific instructions or utilizing human inputs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in this context?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition can be done in common ways such as using prompting techniques like Chain of Thought or Tree of Thoughts to guide models to break down tasks into smaller steps. Additionally, task-specific instructions can be provided to help with decomposition, such as asking for subgoals or outlining steps for achieving a specific task. Human inputs can also be utilized for task decomposition, allowing for a more tailored approach to breaking down complex tasks.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "def text_extractor():\n",
    "    chroma_directory = 'db3/'\n",
    "\n",
    "    db = Chroma(persist_directory=chroma_directory, embedding_function=OpenAIEmbeddings())\n",
    "    db_retriever = db.as_retriever()\n",
    "\n",
    "    return db_retriever\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | text_extractor(),\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='1. Goo Ye Jui\\n2. Jia-Hao Lo\\n3. Ang Teik Hun'),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'})]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Name these candidates one by one.\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Name these candidates one by one.'),\n",
       "  AIMessage(content='1. Goo Ye Jui\\n2. Jia-Hao Lo\\n3. Ang Teik Hun'),\n",
       "  HumanMessage(content='In the scale of 1-10, how would you rate their skill sets as a Data Scientis?'),\n",
       "  AIMessage(content='Based on the information provided, Goo Ye Jui has relevant experience as a Data Science Intern and has a Bachelor of Computer Science in Data Engineering with Honours. Jia-Hao Lo has a Master of Science in Analytics and relevant experience in Data Science Department On-Job Training. Ang Teik Hun has a Master of Applied Data Analytics with a background in Neural Network and Document Analysis. \\n\\nConsidering their education, experience, and skills, Goo Ye Jui would be rated around 7, Jia-Hao Lo around 6, and Ang Teik Hun around 8 as Data Scientists on a scale of 1-10.')]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content=\"Based on the provided information, Goo Ye Jui seems to have a solid foundation in data science with relevant experience in developing machine learning models and working on live projects. Jia-Hao Lo has experience in data science through on-job training and engineering internships. Ang Teik Hun has a strong background in data science with a Master's degree in Applied Data Analytics and relevant experience in using various data mining and machine learning techniques.\\n\\nOn a scale of 1-10, Goo Ye Jui could be rated around 7, Jia-Hao Lo around 5, and Ang Teik Hun around 8 based on their education level, relevant experience, and skills mentioned in the context.\"),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'})]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"In the scale of 1-10, how would you rate their skill sets as a Data Scientis?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content=\"Based on the provided information, the candidates can be ranked based on their skill set score as follows:\\n\\n1. Ang Teik Hun\\n2. Goo Ye Jui\\n3. Jia-Hao Lo\\n\\nAng Teik Hun has a comprehensive skill set that includes a strong background in data analytics, machine learning, neural networks, and various relevant certifications and courses. Goo Ye Jui also has relevant experience in data science and machine learning, but Ang Teik Hun's skill set appears to be more extensive. Jia-Hao Lo has a background in analytics and engineering but may not have as much specific experience in data science and machine learning as the other two candidates.\"),\n",
       " 'docs': [Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.96\\nEducation Background: Bachelor of Computer Science (Data Engineering) With Honours\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Goo Ye Jui\\nphone_number: +6018-4040438\\nemail: yjyejui626@gmail.com\\nlocal: LORONG JAMBU MADU 2, TAMAN JAMBU MADU, 14000 BUKIT MERTAJAM, PENANG\\nlast role: DATA SCIENCE INTERN\\nRelevant experience: Assisted in the development of machine learning model contributing to the automation of critical business processes. Collaborated with a team of data professionals to work on live projects, applying data analytics and machine learning to derive actionable insights and drive innovation. Gained hands-on experience in data science methodologies, tools, and technologies, furthering the journey toward becoming a proficient data scientist. Applied techniques like natural language processing and OpenAI GPT-3 text-davinci-003 engine on the Resume Parser App. Integrated a new feature to automate the process of identifying the candidate’s criteria specified by HR. Improved User Experience for the Resume Parser Flask Application by integrating the email feature. Assisted in operations, achieving production of 2500 board daily. Work within a fast-paced production environment, contributing to the company's mission of producing high-quality solar products. Attention to detail, precision, and adherence to safety protocols. Operate automated machinery and equipment for the production of wafer. Monitor the production process to ensure product quality meets company standards. Strive to maximize production efficiency by minimizing downtime and improving processes.\\nUniversity: UNIVERSITI TEKNOLOGI MALAYSIA\\nCGPA: 3.74\\nEducation Background: Foundation in Science\\n\", metadata={'page': 0, 'source': 'GooYeJui_CV.pdf'}),\n",
       "  Document(page_content=\"name: Ang Teik Hun\\nphone_number: +60124773683\\nemail: teikhun0422@hotmail.com\\nlast role: Internship as Data Scientist, Petroliam Nasional Berhad\\nyears of experience: November 2020 - July 2021\\neducation level: Master of Applied Data Analytics\\nCGPA: 6.42\\nUniversity: Australian National University\\nEducation Background: Neural Network and Document Analysis\\nData Science Background: Major in Neural Network and Document Analysis\\nRelevant experience: Used data mining and machine learning techniques, feature selection, forecasting, clustering, database management, SQL, Power BI, Python, neural network, machine learning, data wrangling, optimization\\nhobbies: Gym and Cooking\\ncertifications: ['Microsoft Certified: Azure Fundamentals', 'Microsoft Certified: Azure AI Engineer Associate']\\nmembership: Institution of Civil Engineers, ICE Board of Engineer Malaysia, Graduate Engineer\\nskills: ['Analytical Thinking Skills', 'Adaptability', 'Time Management', 'Leadership', 'Power BI', 'Python', 'Neural Network', 'Machine Learning (SKlearn)', 'Database SQL', 'Data Wrangling', 'Optimization']\\ncourses: ['Data & AI Essentials, IBM', 'Machine Learning A-Z™: Hands-On Python & R In Data Science, Udemy', '70-778, DA-100: Analyzing and Visualizing Data with Power BI, Udemy', 'Master statistics & machine learning: intuition, math, code, Udemy', 'Data Science Math Skills, Coursera', 'Computer Vision Basic, Coursera']\\npublications: ['Insight on Extraction and Characterization of Biopolymers as the Green Coagulants for Microalgae Harvesting, MDPI Water, May 2020', 'Encyclopedia, Sept 2020']\\nreferences: [{'name': 'A/Prof Qing Wang', 'position': 'School of Computing, The Australian National University', 'contact': '+61(0)261254625', 'email': 'qing.wang@anu.edu.au'}, {'name': 'Dr Vikram Sorathia', 'position': 'Head (Data Science) at PETRONAS', 'contact': '+(60)18-9826762', 'email': 'vikrambhai.sorathia@petronas.com'}]\\nawards: [{'name': 'ANU+ Award', 'duration': 'July 2021 - Dec 2022', 'description': 'Formally recognizes experience and contribution achieved through volunteering. Volunteer 40 hours in computing related field.'}]\\nclubs: ['Head of Ceremony and Protocol department, Medical Camp, Rotaract Club', 'Assistance Head of Club Serve Department, Rotaract Club']\\n\", metadata={'page': 0, 'source': 'Ang Teik Hun Resume.pdf'}),\n",
       "  Document(page_content='name: Jia-Hao Lo\\nphone_number: (872) -810-5588\\nemail: jiahaolo@u.northwestern.edu\\nlocal: Evanston, Illinois\\neducation level: Master of Science in Analytics\\nCGPA: 3.83\\nUniversity: Northwestern University\\nEducation Background: Bachelor of Civil Engineering in Offshore Engineering\\nRelevant experience: Data Science Department On-Job Training, Engineering Internship\\nyears of experience: Nov 2020 – July 2021, May 2019 – Dec 2019\\n', metadata={'page': 0, 'source': 'JiaHao_Lo.pdf'})]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Please rank the candidates based on their skill set score.\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
